{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwiZDCbaURgp"
      },
      "source": [
        "#Using a Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "crl76Mj5PBZV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, Flatten, Input, Concatenate, RepeatVector, Lambda, LSTM, Dot, Softmax\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from transformers import BertTokenizer, TFBertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "RAhLng2GPHlH"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/test.csv\")\n",
        "df.head()\n",
        "df.shape\n",
        "df_train = df[:100]\n",
        "df_train.shape\n",
        "df_test = df[100:110]\n",
        "df_train.head()\n",
        "df_test.shape\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "def clean_text(review):\n",
        "    review = re.sub('[^a-zA-Z0-1]', ' ', review)\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "\n",
        "\n",
        "    review = ' '.join(review)\n",
        "    return review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "xZ65l_wuQgtX"
      },
      "outputs": [],
      "source": [
        "article_train = df_train['article'].values.tolist()\n",
        "highlights_train = df_train['highlights'].values.tolist()\n",
        "article_test = df_test['article'].values.tolist()\n",
        "highlights_test = df_test['highlights'].values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcDusP7uR0qe",
        "outputId": "5d275a02-e474-4eb3-cf37-82c757927525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.3)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow tensorflow-addons transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7X4Pjzq0SG5T",
        "outputId": "a3df93cc-2d14-450a-dda4-92a10a37ceb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.2)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade tensorflow-addons\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfCF2NB3SM_C",
        "outputId": "d2da7efa-71c1-4fd0-fd00-cc823c5a89ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "jBIoOaxqRPSL"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import TFAutoModelForSeq2SeqLM\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using TFIDF vectorizer"
      ],
      "metadata": {
        "id": "6CM5wdA2pl1v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "1a3OrK8HUPWN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e7cac7b-aaae-4b5b-c642-4320336b2b65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPVetclwtwFq",
        "outputId": "6bb5902f-46ab-4730-c253-bd3ebdb3c0bf"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize articles into sentences\n",
        "sentences_train = [word_tokenize(sent.lower()) for article in article_train for sent in sent_tokenize(article)]\n",
        "sentences_test = [word_tokenize(sent.lower()) for article in article_test for sent in sent_tokenize(article)]\n",
        "\n",
        "# Flatten the list of sentences for TF-IDF vectorization\n",
        "flat_sentences_train = [' '.join(sent) for sent in sentences_train]\n",
        "flat_sentences_test = [' '.join(sent) for sent in sentences_test]\n",
        "\n",
        "# Create TF-IDF vectors after removing stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english')  # Set stop_words to 'english'"
      ],
      "metadata": {
        "id": "4BdAI10lsK1c"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_matrix_train = vectorizer.fit_transform(flat_sentences_train)\n",
        "tfidf_matrix_test = vectorizer.transform(flat_sentences_test)"
      ],
      "metadata": {
        "id": "77qwyhfDsM72"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(article, num_words=50):\n",
        "    article_tfidf = vectorizer.transform([article.lower()])\n",
        "    similarity_matrix = cosine_similarity(article_tfidf, tfidf_matrix_train)\n",
        "\n",
        "    # Get the indices of the most important sentences\n",
        "    top_sentences_indices = similarity_matrix.argsort()[0][-num_words:][::-1]\n",
        "\n",
        "    # Sort the sentences based on their original order in the document\n",
        "    top_sentences_indices = sorted(top_sentences_indices)\n",
        "\n",
        "    # Extract the top sentences from the original article\n",
        "    top_sentences = [flat_sentences_train[i] for i in top_sentences_indices]\n",
        "\n",
        "    # Combine the top sentences to form the summary\n",
        "    summary = ' '.join(top_sentences)\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "JeEhH5fVsPJP"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_summaries = [generate_summary(article) for article in article_test]\n",
        "\n",
        "# Evaluate using BLEU metric\n",
        "bleu_scores = [sentence_bleu([ref.split()], gen.split()) for ref, gen in zip(highlights_test, predicted_summaries) if gen]\n",
        "mean_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "print(\"Mean BLEU Score:\", mean_bleu_score)"
      ],
      "metadata": {
        "id": "X4WA18QfsQ05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b52b3b11-b563-4873-8aa6-b4c09a02b8bc"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean BLEU Score: 0.0003756286840992494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using Encoder Decoder Sequence"
      ],
      "metadata": {
        "id": "nvXI_0K9sXal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a larger T5 model\n",
        "model_name_t5 = \"t5-base\"\n",
        "\n",
        "# Or an even larger one\n",
        "model_name_t5 = \"t5-large\"\n",
        "\n",
        "# For the largest T5 model\n",
        "model_name_t5 = \"t5-3.5-turbo\"\n"
      ],
      "metadata": {
        "id": "bcVy6ekvuso5"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "JZ6pzIv7sS0o"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sequences(sequences, tokenizer, max_length):\n",
        "    # Tokenize sequences and convert to numerical IDs\n",
        "    tokenized_sequences = [tokenizer(seq, truncation=True, padding='max_length')['input_ids'] for seq in sequences]\n",
        "\n",
        "    # No need to pad_sequences for BERT-based models\n",
        "    return tokenized_sequences\n"
      ],
      "metadata": {
        "id": "VTPkhkyos0GT"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_articles_train = preprocess_sequences(article_train, tokenizer, max_length_article)\n",
        "tokenized_summaries_train = preprocess_sequences(highlights_train, tokenizer, max_length_summary)\n",
        "tokenized_articles_test = preprocess_sequences(article_test, tokenizer, max_length_article)\n",
        "tokenized_summaries_test = preprocess_sequences(highlights_test, tokenizer, max_length_summary)\n",
        "\n"
      ],
      "metadata": {
        "id": "ixW9WDGFtArn"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have tokenized sequences for both articles and summaries\n",
        "input_ids_train = tokenizer_transformer(article_train, return_tensors=\"tf\", padding=True, truncation=True, max_length=max_length_article)\n",
        "decoder_input_ids_train = tokenizer_transformer(highlights_train, return_tensors=\"tf\", padding=True, truncation=True, max_length=max_length_summary)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JD09AAp4uT4Q"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_transformer.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model_transformer.fit(\n",
        "    {'input_ids': input_ids_train['input_ids'], 'attention_mask': input_ids_train['attention_mask'], 'decoder_input_ids': decoder_input_ids_train['input_ids']},\n",
        "    decoder_input_ids_train['input_ids'],\n",
        "    epochs=50, batch_size=16, validation_split=0.2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVBj_I00s2fu",
        "outputId": "41abf84c-1e0e-4d3d-cea0-e24f7a0a36ed"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "5/5 [==============================] - 72s 2s/step - loss: 10.3782 - accuracy: 0.0122 - val_loss: 10.3775 - val_accuracy: 0.0135\n",
            "Epoch 2/50\n",
            "5/5 [==============================] - 5s 1s/step - loss: 10.3775 - accuracy: 0.0141 - val_loss: 10.3775 - val_accuracy: 0.0135\n",
            "Epoch 3/50\n",
            "5/5 [==============================] - 5s 1s/step - loss: 10.3775 - accuracy: 0.0146 - val_loss: 10.3775 - val_accuracy: 0.0135\n",
            "Epoch 4/50\n",
            "5/5 [==============================] - 5s 981ms/step - loss: 10.3775 - accuracy: 0.0153 - val_loss: 10.3775 - val_accuracy: 0.0135\n",
            "Epoch 5/50\n",
            "5/5 [==============================] - 5s 965ms/step - loss: 10.3775 - accuracy: 0.0136 - val_loss: 10.3775 - val_accuracy: 0.0135\n",
            "Epoch 6/50\n",
            "5/5 [==============================] - 5s 1s/step - loss: 10.3775 - accuracy: 0.0146 - val_loss: 10.3775 - val_accuracy: 0.0135\n",
            "Epoch 7/50\n",
            "5/5 [==============================] - 5s 964ms/step - loss: 10.3775 - accuracy: 0.0150 - val_loss: 10.3775 - val_accuracy: 0.0135\n",
            "Epoch 8/50\n",
            "5/5 [==============================] - 5s 944ms/step - loss: 10.3775 - accuracy: 0.0139 - val_loss: 10.3775 - val_accuracy: 0.0135\n",
            "Epoch 9/50\n",
            "5/5 [==============================] - 5s 984ms/step - loss: 10.3775 - accuracy: 0.0150 - val_loss: 10.3775 - val_accuracy: 0.0135\n",
            "Epoch 10/50\n",
            "5/5 [==============================] - 5s 993ms/step - loss: 10.3775 - accuracy: 0.0134 - val_loss: 10.3775 - val_accuracy: 0.0135\n",
            "Epoch 11/50\n",
            "5/5 [==============================] - 5s 933ms/step - loss: 10.3775 - accuracy: 0.0150 - val_loss: 10.3775 - val_accuracy: 0.0135\n",
            "Epoch 12/50\n",
            "5/5 [==============================] - 5s 927ms/step - loss: 10.3775 - accuracy: 0.0141 - val_loss: 10.3775 - val_accuracy: 0.0135\n",
            "Epoch 13/50\n",
            "5/5 [==============================] - 5s 984ms/step - loss: 10.3775 - accuracy: 0.0159 - val_loss: 10.3775 - val_accuracy: 0.0135\n",
            "Epoch 14/50\n",
            "5/5 [==============================] - 5s 990ms/step - loss: 10.3775 - accuracy: 0.0149 - val_loss: 10.3775 - val_accuracy: 0.0135\n",
            "Epoch 15/50\n",
            "5/5 [==============================] - 5s 934ms/step - loss: 10.3782 - accuracy: 0.0148 - val_loss: 10.3775 - val_accuracy: 0.0135\n",
            "Epoch 16/50\n",
            "5/5 [==============================] - 5s 935ms/step - loss: 10.3775 - accuracy: 0.0150 - val_loss: 10.3775 - val_accuracy: 0.0135\n",
            "Epoch 17/50\n",
            "5/5 [==============================] - 5s 996ms/step - loss: 10.3775 - accuracy: 0.0136 - val_loss: 10.3775 - val_accuracy: 0.0135\n",
            "Epoch 18/50\n",
            "5/5 [==============================] - 5s 944ms/step - loss: 10.3775 - accuracy: 0.0144 - val_loss: 10.3775 - val_accuracy: 0.0135\n",
            "Epoch 19/50\n",
            "5/5 [==============================] - 5s 994ms/step - loss: 10.3781 - accuracy: 0.0100 - val_loss: 10.3775 - val_accuracy: 0.0055\n",
            "Epoch 20/50\n",
            "5/5 [==============================] - 5s 961ms/step - loss: 10.3775 - accuracy: 0.0103 - val_loss: 10.3775 - val_accuracy: 0.0050\n",
            "Epoch 21/50\n",
            "5/5 [==============================] - 5s 999ms/step - loss: 10.3775 - accuracy: 0.0089 - val_loss: 10.3775 - val_accuracy: 0.0045\n",
            "Epoch 22/50\n",
            "5/5 [==============================] - 5s 952ms/step - loss: 10.3775 - accuracy: 0.0109 - val_loss: 10.3775 - val_accuracy: 0.0035\n",
            "Epoch 23/50\n",
            "5/5 [==============================] - 5s 1s/step - loss: 10.3775 - accuracy: 0.0103 - val_loss: 10.3775 - val_accuracy: 0.0035\n",
            "Epoch 24/50\n",
            "5/5 [==============================] - 5s 1s/step - loss: 10.3775 - accuracy: 0.0080 - val_loss: 10.3775 - val_accuracy: 0.0035\n",
            "Epoch 25/50\n",
            "5/5 [==============================] - 5s 1s/step - loss: 10.3775 - accuracy: 0.0084 - val_loss: 10.3775 - val_accuracy: 0.0035\n",
            "Epoch 26/50\n",
            "5/5 [==============================] - 5s 1s/step - loss: 10.3775 - accuracy: 0.0098 - val_loss: 10.3775 - val_accuracy: 0.0035\n",
            "Epoch 27/50\n",
            "5/5 [==============================] - 5s 954ms/step - loss: 10.3775 - accuracy: 0.0096 - val_loss: 10.3775 - val_accuracy: 0.0035\n",
            "Epoch 28/50\n",
            "5/5 [==============================] - 5s 1s/step - loss: 10.3775 - accuracy: 0.0086 - val_loss: 10.3775 - val_accuracy: 0.0035\n",
            "Epoch 29/50\n",
            "5/5 [==============================] - 5s 996ms/step - loss: 10.3775 - accuracy: 0.0101 - val_loss: 10.3775 - val_accuracy: 0.0035\n",
            "Epoch 30/50\n",
            "5/5 [==============================] - 5s 997ms/step - loss: 10.3775 - accuracy: 0.0094 - val_loss: 10.3775 - val_accuracy: 0.0035\n",
            "Epoch 31/50\n",
            "5/5 [==============================] - 5s 994ms/step - loss: 10.3775 - accuracy: 0.0089 - val_loss: 10.3775 - val_accuracy: 0.0035\n",
            "Epoch 32/50\n",
            "5/5 [==============================] - 5s 939ms/step - loss: 10.3775 - accuracy: 0.0093 - val_loss: 10.3775 - val_accuracy: 0.0035\n",
            "Epoch 33/50\n",
            "5/5 [==============================] - 5s 946ms/step - loss: 10.3775 - accuracy: 0.0105 - val_loss: 10.3775 - val_accuracy: 0.0035\n",
            "Epoch 34/50\n",
            "5/5 [==============================] - 5s 996ms/step - loss: 10.3775 - accuracy: 0.0104 - val_loss: 10.3775 - val_accuracy: 0.0035\n",
            "Epoch 35/50\n",
            "5/5 [==============================] - 5s 990ms/step - loss: 10.3775 - accuracy: 0.0096 - val_loss: 10.3775 - val_accuracy: 0.0035\n",
            "Epoch 36/50\n",
            "5/5 [==============================] - 5s 988ms/step - loss: 10.3775 - accuracy: 0.0105 - val_loss: 10.3775 - val_accuracy: 0.0035\n",
            "Epoch 37/50\n",
            "5/5 [==============================] - 5s 968ms/step - loss: 10.3775 - accuracy: 0.0105 - val_loss: 10.3775 - val_accuracy: 0.0035\n",
            "Epoch 38/50\n",
            "5/5 [==============================] - 5s 995ms/step - loss: 10.3775 - accuracy: 0.0105 - val_loss: 10.3775 - val_accuracy: 0.0035\n",
            "Epoch 39/50\n",
            "5/5 [==============================] - 5s 999ms/step - loss: 10.3775 - accuracy: 0.0095 - val_loss: 10.3775 - val_accuracy: 0.0035\n",
            "Epoch 40/50\n",
            "5/5 [==============================] - 5s 1s/step - loss: 10.3775 - accuracy: 0.0096 - val_loss: 10.3775 - val_accuracy: 0.0035\n",
            "Epoch 41/50\n",
            "5/5 [==============================] - 5s 1s/step - loss: 10.3775 - accuracy: 0.0105 - val_loss: 10.3775 - val_accuracy: 0.0035\n",
            "Epoch 42/50\n",
            "5/5 [==============================] - 5s 945ms/step - loss: 10.3775 - accuracy: 0.0113 - val_loss: 10.3775 - val_accuracy: 0.0035\n",
            "Epoch 43/50\n",
            "5/5 [==============================] - 5s 992ms/step - loss: 10.3775 - accuracy: 0.0117 - val_loss: 10.3775 - val_accuracy: 0.0035\n",
            "Epoch 44/50\n",
            "5/5 [==============================] - 5s 1s/step - loss: 10.3775 - accuracy: 0.0098 - val_loss: 10.3775 - val_accuracy: 0.0035\n",
            "Epoch 45/50\n",
            "5/5 [==============================] - 5s 945ms/step - loss: 10.3775 - accuracy: 0.0104 - val_loss: 10.3775 - val_accuracy: 0.0035\n",
            "Epoch 46/50\n",
            "5/5 [==============================] - 5s 939ms/step - loss: 10.3775 - accuracy: 0.0095 - val_loss: 10.3775 - val_accuracy: 0.0035\n",
            "Epoch 47/50\n",
            "5/5 [==============================] - 5s 1000ms/step - loss: 10.3775 - accuracy: 0.0093 - val_loss: 10.3775 - val_accuracy: 0.0035\n",
            "Epoch 48/50\n",
            "5/5 [==============================] - 5s 1000ms/step - loss: 10.3775 - accuracy: 0.0117 - val_loss: 10.3775 - val_accuracy: 0.0035\n",
            "Epoch 49/50\n",
            "5/5 [==============================] - 5s 948ms/step - loss: 10.3775 - accuracy: 0.0103 - val_loss: 10.3775 - val_accuracy: 0.0035\n",
            "Epoch 50/50\n",
            "5/5 [==============================] - 5s 997ms/step - loss: 10.3775 - accuracy: 0.0095 - val_loss: 10.3775 - val_accuracy: 0.0035\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c7a0cb58c70>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids_test = tokenizer_transformer(article_test, return_tensors=\"tf\", padding=True, truncation=True, max_length=max_length_article)\n"
      ],
      "metadata": {
        "id": "jkYLJLVywHCo"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_summaries = []\n",
        "for input_seq in input_ids_test['input_ids']:\n",
        "    # Use the trained model to generate summaries\n",
        "    input_tensor = tf.constant(input_seq[tf.newaxis, :])\n",
        "    attention_mask = tf.ones_like(input_tensor)\n",
        "    predicted_ids = model_transformer.generate(input_tensor, attention_mask=attention_mask, max_length=max_length_summary, num_beams=4, length_penalty=2.0, early_stopping=True)\n",
        "\n",
        "    # Decode the generated IDs to text\n",
        "    predicted_summary = tokenizer_transformer.decode(predicted_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    predicted_summaries.append(predicted_summary)\n",
        "\n",
        "# Now 'predicted_summaries' contains the generated summaries for your test articles\n"
      ],
      "metadata": {
        "id": "LNPXqPCjs4NC"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(predicted_summaries)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MSfrdKK3rP-",
        "outputId": "dfef58e8-8b6d-47bd-d057-39d25672ece8"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "It0FbJp07vB5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
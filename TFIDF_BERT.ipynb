{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQrnhyMB2s2Z",
        "outputId": "67fd97e9-f89e-4f9d-efb0-abecb940c11f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.translate.bleu_score import sentence_bleu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/test.csv\")\n",
        "df.head()\n",
        "df.shape\n",
        "df_train = df[:500]\n",
        "df_train.shape\n",
        "df_test = df[500:600]\n",
        "df_train.head()\n",
        "df_test.shape\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import re\n",
        "ps = PorterStemmer()\n",
        "def clean_text(review):\n",
        "    review = re.sub('[^a-zA-Z0-1]', ' ', review)\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "\n",
        "\n",
        "    review = ' '.join(review)\n",
        "    return review"
      ],
      "metadata": {
        "id": "a1T5e98u258S"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_train = df_train['article'].apply(clean_text).values.tolist()\n",
        "highlights_train = df_train['highlights'].apply(clean_text).values.tolist()\n",
        "article_test = df_test['article'].apply(clean_text).values.tolist()\n",
        "highlights_test = df_test['highlights'].apply(clean_text).values.tolist()"
      ],
      "metadata": {
        "id": "f5N_CzGd28DI"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#article_train = df_train['article'].values.tolist()\n",
        "#highlights_train = df_train['highlights'].values.tolist()\n",
        "#article_test = df_test['article'].values.tolist()\n",
        "#highlights_test = df_test['highlights'].values.tolist()"
      ],
      "metadata": {
        "id": "fV-NILmE3RVv"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ctz_gngw4sxa",
        "outputId": "a8f48c82-7c50-4503-8ac9-326632c9dd8c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "# Assuming you have already processed your data and created a list of articles\n",
        "# For simplicity, let's assume article_train and article_test are already prepared\n",
        "\n",
        "# Tokenize articles into sentences\n",
        "sentences_train = [word_tokenize(sent.lower()) for article in article_train for sent in sent_tokenize(article)]\n",
        "sentences_test = [word_tokenize(sent.lower()) for article in article_test for sent in sent_tokenize(article)]\n",
        "\n",
        "# Flatten the list of sentences for TF-IDF vectorization\n",
        "flat_sentences_train = [' '.join(sent) for sent in sentences_train]\n",
        "flat_sentences_test = [' '.join(sent) for sent in sentences_test]\n",
        "\n",
        "# Create TF-IDF vectors after removing stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english')  # Set stop_words to 'english'\n",
        "tfidf_matrix_train = vectorizer.fit_transform(flat_sentences_train)\n",
        "tfidf_matrix_test = vectorizer.transform(flat_sentences_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HR1tU4En9DYY",
        "outputId": "5482c65a-cc8c-433b-ed4f-a4718f710fb9"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate summaries using TF-IDF\n",
        "def generate_summary(article, num_words=50):\n",
        "    article_tfidf = vectorizer.transform([article.lower()])\n",
        "\n",
        "    # Get the indices of the most important sentences based on TF-IDF values\n",
        "    important_sentence_indices = article_tfidf.indices\n",
        "\n",
        "    # Filter out indices that are out of range\n",
        "    important_sentence_indices = [i for i in important_sentence_indices if i < len(flat_sentences_train)]\n",
        "\n",
        "    # Sort the remaining indices\n",
        "    important_sentence_indices = sorted(important_sentence_indices)\n",
        "\n",
        "    # Extract the important sentences from the original article\n",
        "    important_sentences = [flat_sentences_train[i] for i in important_sentence_indices]\n",
        "\n",
        "    # Adjust the number of words based on the available sentences\n",
        "    num_words = min(num_words, len(important_sentence_indices))\n",
        "\n",
        "    # Combine the important sentences to form the summary\n",
        "    summary = ' '.join(important_sentences[:num_words])\n",
        "\n",
        "    return summary\n",
        "\n",
        "# Generate summaries for the test set\n",
        "predicted_summaries = [generate_summary(article) for article in article_test]"
      ],
      "metadata": {
        "id": "Qch49ZGM9Gcd"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate cosine similarity\n",
        "cosine_similarities = []\n",
        "for actual, predicted in zip(highlights_test, predicted_summaries):\n",
        "    actual_tfidf = vectorizer.transform([actual.lower()])\n",
        "    predicted_tfidf = vectorizer.transform([predicted.lower()])\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    cosine_sim = cosine_similarity(actual_tfidf, predicted_tfidf)[0][0]\n",
        "    cosine_similarities.append(cosine_sim)\n",
        "\n",
        "# Mean cosine similarity\n",
        "mean_cosine_similarity = sum(cosine_similarities) / len(cosine_similarities)\n",
        "print(\"Mean Cosine Similarity:\", mean_cosine_similarity)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqtpArol9M3J",
        "outputId": "9e614fef-ebe7-415c-c5d9-b4ec014a984e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Cosine Similarity: 0.021815581391597116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "74zQ1ixI949O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}